---
title: "P8105_hw6_zf2211"
author: "Francis"
date: "11/25/2018"
output: github_document
---



```{r}
# Load packages
library(tidyverse)
library(rvest)
library(modelr)
library(mgcv)
```

# Problem 1



```{r}
# read data
homicide <- read.csv("./data/homicide-data.csv")
```

```{r}
#Create a city_state variable
homicide <- 
  homicide %>% 
  mutate(city_state = str_c(city, ",", state)) %>% 
  select(-city, -state)

# remove city_state, solved is num. 
homicide <- 
  homicide %>% 
  filter(!city_state %in% c("Dallas,TX", "Phoenix,AZ", "Kansas City,MO", "Tulsa,AL")) %>% 
  mutate(solved = as.numeric(disposition == "Closed by arrest"))
```


```{r}
# modifiy victim_race
homicide  <-  
  homicide %>% 
  mutate(victim_race = as_factor(ifelse(victim_race == "White", "white","non-white"))) %>% 
  mutate(victim_race = fct_relevel(victim_race, "white")) %>% 
  mutate(victim_age = as.numeric(victim_age)) 
```


```{r}
### logistic regression
# fit and save
fit_logis <-  
  homicide %>% 
  filter(city_state == "Baltimore,MD") %>% 
  glm(solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) 

# obtain the estimate and confidence interval of the adjusted odds ratio for Baltimore,MD
tibble_left <-  
  fit_logis %>% 
  broom::tidy() %>% 
  mutate(OR_estimate = exp(estimate)) %>% 
  select(term, OR_estimate, p.value)
tibble_right = as_tibble(exp(confint.default(fit_logis)))
cbind(tibble_left, tibble_right) %>% 
  knitr::kable(digits = 3)
```



```{r}
# run glm for every city

glm_every <-  
  homicide %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(models = map(data, ~glm(solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())), 
         results = map(.x = models, ~ broom::tidy(.x, exponentiate = TRUE, conf.int = TRUE))) %>% 
  select(-data, -models) %>% 
  unnest() %>% 
  filter(term == "victim_racenon-white") %>% 
  select(city_state, OR = estimate, conf.low, conf.high)
  
glm_every %>% 
  head() %>% 
  knitr::kable(digits = 3)
```





```{r}
# plot estimated ORs and CIs
glm_every %>% 
  mutate(city_state = as.factor(city_state), 
         city_state = fct_reorder(city_state, desc(OR))) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, size = rel(0.8), hjust = 1)) +
  labs(
    title = "Variation of OR with cities",
    x = "City/State",
    y = "OR"
  )
```


For most of the cities, the estimates for odds ratio(OR) are below 1.0(1.0 are not included in their 95% CI). So, comparing to white victims for those cities, there is significantly lower estimated odds of solving homicides among non-white victims. The CI for some cities (such as Boston,MA) are narrow, so be can be more confident in future predictions to these cities.





# Problem 2



```{r}
# read data
bwt <- read_csv("./data/birthweight.csv")
```




```{r}
#tidy data
bwt <-  
  bwt %>% 
  mutate(babysex = babysex -1, # male = 0, female = 1
         babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))
# missing values
bwt[which(!complete.cases(bwt)),]           
```

```{r}
# correlation matrix
bwt %>% 
  select(-babysex, -frace, -mrace, -malform) %>% 
  cor()
```


Firstly we compute a correlation matrix to check whether there is potential interactions between varaibles in this birthweight data. As seen from the output, there is a noticable positive correlation between `bhead` and `blength`, we will include later. 



All other variables, except `parity` and `smoken`, show moderate correlation with `bw`--the outcome of interest. In addition, since `pnumlbw` and `pnumsga` take the value 0 for the entire dataset,  "NA"s were returned. 


Because `ppbmi` is calculated from `mheight` and `ppwt` and `wtgain` refers to the difference between `ppwt` and `delwt`, so only `wtgain` left in our initial model.



So we will start our model selection procedure with a linear model that has variables except `babysex`, `bhead`, `blength`, `fincome`, `gaweeks`, `malform`, `menarche`, `momage`, `mrace` and `wtgain` as predictors. Then we will use backward elimination method to remove non-significant variables that have p-value less than 0.05.



```{r}
###Backward elimination
# initial model
mult.fit <- lm(bwt ~ babysex + bhead + blength + fincome + gaweeks + malform+ menarche + momage+ mrace + wtgain, data = bwt)
summary(mult.fit)

# add interaction bhead and blength
mult.fit <- lm(bwt ~ babysex + bhead + blength + fincome + gaweeks + malform+ menarche + momage+ mrace + wtgain + bhead*blength, data = bwt)
summary(mult.fit) 

# retain variables that p < 0.05
mult.fit <- lm(bwt ~ babysex + bhead + blength  + gaweeks + mrace + wtgain + bhead*blength, data = bwt)
summary(mult.fit)
```

Then the final model we obtain is `bw ~ babysex + bhead + blength  + gaweeks + mrace + wtgain + bhead*blength`(overall R-squared of 0.71 and the p-value of overall F test being significantly less than 0.05.)


```{r}
## plot residual vs. fitted values
bwt %>% 
  add_predictions(mult.fit) %>% 
  add_residuals(mult.fit) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5)
```


```{r}
# compare models
fit1 <- lm(bwt ~ blength + gaweeks, data = bwt)
fit2 <- lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = bwt)

```

```{r}
### Three-way interaction
#generate a random cross validation dataframe
set.seed(777)
cv_df <- crossv_mc(bwt, 10) 

#convert cv_df to a tibble
cv_df <- 
  cv_df %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

# fits each of the candidate models to cv datasets
cv_df <-  
  cv_df %>% 
  mutate(our_mod  = map(train, ~lm(bwt ~ babysex + bhead + blength  + gaweeks + mrace + wtgain + bhead*blength, data = .x)),
         fit1_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         fit2_mod = map(train, ~lm(bwt ~ bhead+ blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_our = map2_dbl(our_mod, test, ~rmse(model = .x, data = .y)),
         rmse_fit1 = map2_dbl(fit1_mod, test, ~rmse(model = .x, data = .y)),
         rmse_fit2 = map2_dbl(fit2_mod, test, ~rmse(model = .x, data = .y)))

# plot the prediction error distribution for each candidate model
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```



As we can see, the second model has the highest overall RMSE, because it is too simple. 


The third model has a lower RMSE, because it takes into account the interaction among bhead, blength, and babysex. 


The model we proposed has the lowest overall distribution of RMSE, showing best prediction accuracy among three.
